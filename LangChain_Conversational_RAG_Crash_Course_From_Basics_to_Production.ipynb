{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipaGSusmWrJS"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade langchain langchain-openai langchain-core langchain_community docx2txt pypdf langchain_chroma sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "envLFUgmZvad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Retrieval Augmented Generation\n",
        "\n",
        "RAG is a technique that enhances language models by cobining them with a retrieval system. It allows the model to access and utilize external knowledge when generating responses."
      ],
      "metadata": {
        "id": "euBE9LMOXSPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "print(langchain.__version__)"
      ],
      "metadata": {
        "id": "kxq8ntQLXRxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'YOUR OPENAI API KEY'"
      ],
      "metadata": {
        "id": "uCOUFiEoZ1km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR LANGCHAIN API KEY\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-course\""
      ],
      "metadata": {
        "id": "lGHWpeokZ1hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kdf0Hzz-Z1Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Call LLM"
      ],
      "metadata": {
        "id": "xL_Tm6AkcDlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "llm_response = llm.invoke(\"Tell me a joke\")\n",
        "\n",
        "llm_response"
      ],
      "metadata": {
        "id": "pEOJb5KZZ0zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "idz3ePMBceaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Parsing Output"
      ],
      "metadata": {
        "id": "4qiYV6TScerP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "output_parser.invoke(llm_response)"
      ],
      "metadata": {
        "id": "mfPBGleQZ0wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bVNnasGaZ0t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Simple Chain"
      ],
      "metadata": {
        "id": "RSUjlIOZc6iE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = llm | output_parser\n",
        "chain.invoke(\"Tell me a joke\")"
      ],
      "metadata": {
        "id": "pt4vU05KXRut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c6F7M2r7XRsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Structured Output"
      ],
      "metadata": {
        "id": "fjk_DfqIecGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class MobileReview(BaseModel):\n",
        "    phone_model: str = Field(description=\"Name and model of the phone\")\n",
        "    rating: float = Field(description=\"Overall rating out of 5\")\n",
        "    pros: List[str] = Field(description=\"List of positive aspects\")\n",
        "    cons: List[str] = Field(description=\"List of negative aspects\")\n",
        "    summary: str = Field(description=\"Brief summary of the review\")\n",
        "\n",
        "review_text = \"\"\"\n",
        "Just got my hands on the new Galaxy S21 and wow, this thing is slick! The screen is gorgeous,\n",
        "colors pop like crazy. Camera's insane too, especially at night - my Insta game's never been\n",
        "stronger. Battery life's solid, lasts me all day no problem.\n",
        "\n",
        "Not gonna lie though, it's pretty pricey. And what's with ditching the charger? C'mon Samsung.\n",
        "Also, still getting used to the new button layout, keep hitting Bixby by mistake.\n",
        "\n",
        "Overall, I'd say it's a solid 4 out of 5. Great phone, but a few annoying quirks keep it from\n",
        "being perfect. If you're due for an upgrade, definitely worth checking out!\n",
        "\"\"\"\n",
        "\n",
        "structured_llm = llm.with_structured_output(MobileReview)\n",
        "output = structured_llm.invoke(review_text)\n",
        "output"
      ],
      "metadata": {
        "id": "nops2IeYXRpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.pros"
      ],
      "metadata": {
        "id": "DB_vwkgJehU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.cons"
      ],
      "metadata": {
        "id": "By9bnKBThQQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NphuU2zxhQNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prompt Template"
      ],
      "metadata": {
        "id": "mt7ytutWhfPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
        "prompt.invoke({\"topic\": \"programming\"})"
      ],
      "metadata": {
        "id": "7dGzuDmwhQKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm | output_parser\n",
        "chain.invoke({\"topic\": \"politicians\"})"
      ],
      "metadata": {
        "id": "V8jALndfhQHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Put all things together\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Define the prompt\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
        "\n",
        "# Intialize the LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Define the output parser\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Compose the chain\n",
        "chain = prompt | llm | output_parser\n",
        "\n",
        "# Use the chain\n",
        "result = chain.invoke({\"topic\": \"programming\"})\n",
        "print(result)"
      ],
      "metadata": {
        "id": "GjKW6FVReh7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cPwsF8oSeh4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LLM Messages"
      ],
      "metadata": {
        "id": "nAg8WS6Vk1E_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "system_message = SystemMessage(content=\"You are a helpful assistant that tells jokes.\")\n",
        "human_message = HumanMessage(content=\"Tell me about programming\")\n",
        "llm.invoke([system_message, human_message])"
      ],
      "metadata": {
        "id": "4icq92M3lE83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful assistant that tells jokes.\"),\n",
        "    (\"human\", \"Tell me about {topic}\")\n",
        "])\n",
        "\n",
        "prompt_value = template.invoke(\n",
        "    {\n",
        "        \"topic\": \"programming\"\n",
        "    }\n",
        ")\n",
        "\n",
        "prompt_value"
      ],
      "metadata": {
        "id": "IdtzmOj7lx8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "13wZF81bl-Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that tells jokes.\"),\n",
        "    HumanMessage(content=\"Tell me about programming\")\n",
        "]\n",
        "response = llm.invoke(messages)\n",
        "print(response)\n",
        "\n",
        "template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful assistant that tells jokes.\"),\n",
        "    (\"human\", \"Tell me about {topic}\")\n",
        "])\n",
        "chain = template | llm\n",
        "response = chain.invoke({\"topic\": \"programming\"})\n",
        "print(response)"
      ],
      "metadata": {
        "id": "V3NOf1qMeh1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W7JwSEAnehSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing the Document"
      ],
      "metadata": {
        "id": "4NRrF2PzneC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from typing import List\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=700,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len\n",
        ")\n",
        "docx_loader = Docx2txtLoader(\"/content/GreenGrow Innovations_Company History.docx\")\n",
        "documents = docx_loader.load()\n",
        "\n",
        "splits = text_splitter.split_documents(documents)\n",
        "print(f\"Split the documents into {len(splits)} chunks.\")"
      ],
      "metadata": {
        "id": "3uzi62b6ehPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0]"
      ],
      "metadata": {
        "id": "4iaz9ajW5vbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits[0]"
      ],
      "metadata": {
        "id": "-DdcDlgI5r0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits[1]"
      ],
      "metadata": {
        "id": "hBUUCV-15rxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lm8d_op64s-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to load documents from a folder\n",
        "def load_documents(folder_path: str) -> List[Document]:\n",
        "    documents = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        if filename.endswith('.pdf'):\n",
        "            loader = PyPDFLoader(file_path)\n",
        "        elif filename.endswith('.docx'):\n",
        "            loader = Docx2txtLoader(file_path)\n",
        "        else:\n",
        "            print(f\"Unsupported file type: {filename}\")\n",
        "            continue\n",
        "        documents.extend(loader.load())\n",
        "    return documents\n",
        "\n",
        "folder_path = \"/content\"\n",
        "documents = load_documents(folder_path)\n",
        "print(f\"Loaded {len(documents)} documents from the folder.\")\n",
        "splits = text_splitter.split_documents(documents)\n",
        "print(f\"Split the documents into {len(splits)} chunks.\")"
      ],
      "metadata": {
        "id": "pXLCw74nehMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# 4. Embedding Documents\n",
        "\n",
        "document_embeddings = embeddings.embed_documents([split.page_content for split in splits])\n",
        "\n",
        "print(f\"Created embeddings for {len(document_embeddings)} document chunks.\")"
      ],
      "metadata": {
        "id": "0vG8_Qe_ehKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(document_embeddings[0])"
      ],
      "metadata": {
        "id": "aJmvKqxw7sd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_embeddings[0]"
      ],
      "metadata": {
        "id": "l-weBOdtehHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence_transformers"
      ],
      "metadata": {
        "id": "yixbAa5G8IjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install sentence_transformers\n",
        "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "document_embeddings = embedding_function.embed_documents([split.page_content for split in splits])\n",
        "document_embeddings[0]"
      ],
      "metadata": {
        "id": "a8av-UfG7-x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fGei7cAc7-un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create and persist Chroma vector store"
      ],
      "metadata": {
        "id": "A0Idq7ew8i2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "collection_name = \"my_collection\"\n",
        "vectorstore = Chroma.from_documents(\n",
        "    collection_name=collection_name,\n",
        "    documents=splits,\n",
        "    embedding=embedding_function,\n",
        "    persist_directory=\"./chroma_db\"\n",
        ")\n",
        "print(\"Vector store created and persisted to './chroma_db'\")"
      ],
      "metadata": {
        "id": "SusKwe2I8fpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Perform similarity Search\n",
        "\n",
        "query = \"When was GreenGrow Innovations founded?\"\n",
        "search_results = vectorstore.similarity_search(query, k=2)\n",
        "print(f\"\\nTop 2 most relevant chunks for the query: '{query}'\\n\")\n",
        "for i, result in enumerate(search_results, 1):\n",
        "    print(f\"Result {i}:\")\n",
        "    print(f\"Source: {result.metadata.get('source', 'Unknown')}\")\n",
        "    print(f\"Content: {result.page_content}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "9AzTrNT19cgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "retriever_results = retriever.invoke(\"When was GreenGrow Innovations founded?\")\n",
        "print(retriever_results)"
      ],
      "metadata": {
        "id": "tu5_qiNz9l_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "Question: {question}\n",
        "Answer: \"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "LhX9lLac-jSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"When was GreenGrow Innovations founded?\")"
      ],
      "metadata": {
        "id": "5iZ_fmEJ-xJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def docs2str(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ],
      "metadata": {
        "id": "RE7O5UgF_Sjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()} | prompt\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"When was GreenGrow Innovations founded?\")"
      ],
      "metadata": {
        "id": "n6ZB1ALrAmKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = (\n",
        "    {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "question = \"When was GreenGrow Innovations founded?\"\n",
        "response = rag_chain.invoke(question)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "3JbeFrfB-jPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ry79yseZCDWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversational RAG"
      ],
      "metadata": {
        "id": "HCJYXsCDCEKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Handling Follow Up Questions"
      ],
      "metadata": {
        "id": "XDC1SxxPCRKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example conversation\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "chat_history = []\n",
        "chat_history.extend([\n",
        "    HumanMessage(content=question),\n",
        "    AIMessage(content=response)\n",
        "])"
      ],
      "metadata": {
        "id": "o_ipZ5d9CanJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "id": "TnLR___7C2kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-7-ZKFDEC2g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "contextualize_q_system_prompt = \"\"\"\n",
        "Given a chat history and the latest user question\n",
        "which might reference context in the chat history,\n",
        "formulate a standalone question which can be understood\n",
        "without the chat history. Do NOT answer the question,\n",
        "just reformulate it if needed and otherwise return it as is.\n",
        "\"\"\"\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "contextualize_chain = contextualize_q_prompt | llm | StrOutputParser()\n",
        "print(contextualize_chain.invoke({\"input\": \"Where is it headquartered?\", \"chat_history\": chat_history}))"
      ],
      "metadata": {
        "id": "KR5mds7sCDTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, contextualize_q_prompt\n",
        ")\n",
        "\n",
        "history_aware_retriever.invoke({\"input\": \"Where it is headquartered?\", \"chat_history\": chat_history})"
      ],
      "metadata": {
        "id": "HnOr_-tvR43F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what if we call the retriver directly (not the history-aware retriever)\n",
        "retriever.invoke(\"Where it is headquartered?\")"
      ],
      "metadata": {
        "id": "PpA_0qYtYqDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant. Use the following context to answer the user's question.\"),\n",
        "    (\"system\", \"Context: {context}\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
      ],
      "metadata": {
        "id": "QPly92fdCDQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.invoke({\"input\": \"Where it is headquartered?\", \"chat_history\": chat_history})"
      ],
      "metadata": {
        "id": "x7Q-HpA4CDOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1yZAaG2rCDLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nWCjz98OCDIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Multi User Chatbot"
      ],
      "metadata": {
        "id": "ctMsfip9cY8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from datetime import datetime\n",
        "\n",
        "DB_NAME = \"rag_app.db\"\n",
        "\n",
        "def get_db_connection():\n",
        "    conn = sqlite3.connect(DB_NAME)\n",
        "    conn.row_factory = sqlite3.Row\n",
        "    return conn\n",
        "\n",
        "def create_application_logs():\n",
        "    conn = get_db_connection()\n",
        "    conn.execute('''CREATE TABLE IF NOT EXISTS application_logs\n",
        "                    (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    session_id TEXT,\n",
        "                    user_query TEXT,\n",
        "                    gpt_response TEXT,\n",
        "                    model TEXT,\n",
        "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')\n",
        "    conn.close()\n",
        "\n",
        "def insert_application_logs(session_id, user_query, gpt_response, model):\n",
        "    conn = get_db_connection()\n",
        "    conn.execute('INSERT INTO application_logs (session_id, user_query, gpt_response, model) VALUES (?, ?, ?, ?)',\n",
        "                 (session_id, user_query, gpt_response, model))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def get_chat_history(session_id):\n",
        "    conn = get_db_connection()\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute('SELECT user_query, gpt_response FROM application_logs WHERE session_id = ? ORDER BY created_at', (session_id,))\n",
        "    messages = []\n",
        "    for row in cursor.fetchall():\n",
        "        messages.extend([\n",
        "            {\"role\": \"human\", \"content\": row['user_query']},\n",
        "            {\"role\": \"ai\", \"content\": row['gpt_response']}\n",
        "        ])\n",
        "    conn.close()\n",
        "    return messages\n",
        "\n",
        "# Initialize the database\n",
        "create_application_logs()"
      ],
      "metadata": {
        "id": "d-KvfVqLCDDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "\n",
        "session_id = str(uuid.uuid4())\n",
        "chat_history = get_chat_history(session_id)\n",
        "print(chat_history)\n",
        "question1 = \"When was GreenGrow Innovations founded?\"\n",
        "answer1 = rag_chain.invoke({\"input\": question1, \"chat_history\": chat_history})['answer']\n",
        "insert_application_logs(session_id, question1, answer1, \"gpt-4-o-mini\")\n",
        "print(f\"Human: {question1}\")\n",
        "print(f\"AI: {answer1}\\n\")"
      ],
      "metadata": {
        "id": "55RH8lgmctWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question2 = \"Where it is headquartered?\"\n",
        "chat_history = get_chat_history(session_id)\n",
        "print(chat_history)\n",
        "answer2 = rag_chain.invoke({\"input\": question2, \"chat_history\": chat_history})['answer']\n",
        "insert_application_logs(session_id, question2, answer2, \"gpt-3.5-turbo\")\n",
        "print(f\"Human: {question2}\")\n",
        "print(f\"AI: {answer2}\\n\")"
      ],
      "metadata": {
        "id": "mkd2_EoGctTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hnJ7MbuRctRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Now, New User came in the scene"
      ],
      "metadata": {
        "id": "zWFLWyXYlJnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "session_id = str(uuid.uuid4())\n",
        "question = \"What is GreenGrow\"\n",
        "chat_history = get_chat_history(session_id)\n",
        "print(chat_history)\n",
        "answer = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})['answer']\n",
        "insert_application_logs(session_id, question, answer, \"gpt-3.5-turbo\")\n",
        "print(f\"Human: {question}\")\n",
        "print(f\"AI: {answer}\\n\")"
      ],
      "metadata": {
        "id": "lHY8ehAQctOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JAsqVQ278fiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gJCUC3gZXRm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part-2 Moving to the Production"
      ],
      "metadata": {
        "id": "Ik-4enEOZymR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M5xjgfF6Zz8z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}